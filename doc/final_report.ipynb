{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import altair as alt\n",
    "alt.data_transformers.enable('json')\n",
    "#alt.renderers.enable('notebook')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from docopt import docopt\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading results from the output files\n",
    "evaluation_matrix = pd.read_csv(\"../results/accuracies.csv\")\n",
    "#evaluation_matrix_base = pd.read_csv(\"../results_baseline//accuracies.csv\")\n",
    "#head = pd.read_csv(\"../results/head.csv\")\n",
    "summary=pd.read_csv(\"../results/num_describe.csv\", index_col=0).applymap(lambda x: '%.2f' %x)\n",
    "test_accuracy = round(evaluation_matrix.iloc[0][2],2)\n",
    "test_accuracy_base = round(evaluation_matrix.iloc[0][1],2)\n",
    "recall = round(evaluation_matrix.iloc[2][2],2)\n",
    "recall_base = round(evaluation_matrix.iloc[2][1],2)\n",
    "precision = round(evaluation_matrix.iloc[3][2],2)\n",
    "precision_base = round(evaluation_matrix.iloc[3][1],2)\n",
    "auc = round(evaluation_matrix.iloc[4][2],2)\n",
    "auc_base = round(evaluation_matrix.iloc[4][1],2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Table of Content:**\n",
    "* Summary\n",
    "* Introduction\n",
    "* Methods\n",
    "* Results\n",
    "* Conclusions\n",
    "* References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "auc": "0.72",
     "precision": "0.39",
     "recall": "0.63",
     "test_accuracy": "0.7"
    }
   },
   "source": [
    "# 1. Summary <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "In this project we try to find the best features that best predict default customers using machine learning tools. Logestic Regression was found to achieve acceptable results on the test data provided to the trained model. The accuracy of the model on test data was about {{test_accuracy}} and the recall on test data found to be {{recall}}. The precision for the model on the test was about {{precision}} .The area under the ROC Curve for the final model is {{auc}}.\n",
    "\n",
    "Due to the risk associated with customers failing to pay, the model was designed to maximize the recall rate, identifing customers that will default to the greatest extent. This was also balanced with the overall accuracy on the training and test dataset. The model predict the following 7 features to be the most important features to predict customers default.\n",
    "\n",
    "1. Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
    "2. EDUCATION\n",
    "3. MARRIAGE\n",
    "4. AGE\n",
    "5. Past monthly repayment status in September 2005 (which is the most recent month before the prediction month)\n",
    "6. Past monthly repayment status in August 2005 (which is the second from the most recent month before the predition month)\n",
    "7. Amount of previous payment (NT dollar) in September 2005 (which is the most recent month before the prediction month)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introduction <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "Prediction of customers default behaviour is critically important in Risk Management by lenders. In particular,  there has been a significant interest in identifying features that are associated with the highest prediction power to reduce the overall lender's credit risk. In this study, we perform a data-informed analysis to build a model that can successfully capture features that predict default payment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Methods <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "## Data\n",
    "We used credit default data collected from the Taiwanese market in 2005. The Data Set is available from [UCI Machine Learning Repository Irvine, CA: University of California, School of Information and Computer Science](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients). The data that contains 23 features from 30,000 customers. was originally publicized by Chung Hua University of Taiwan and Tamkang University of Taiwan. Features include :\n",
    "\n",
    "- `LIMIT_BAL`: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit. \n",
    "- `SEX`: Gender(1 = male; 2 = female).\n",
    "- `EDUCATION`: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). \n",
    "- `MARRIAGE`: Marital status (1 = married; 2 = single; 3 = others).  \n",
    "- `AGE`: Age (year).  \n",
    "- `PAY_1`, `PAY_2`, ..., `PAY_6`: Past monthly repayment status in September 2005, August 2005, ..., April 2005 respectively. ( -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.)  \n",
    "- `BILL_AMT1`, `BILL_AMT2`, ..., `BILL_AMT6`: Amount of bill statement (NT dollar) in September 2005, August 2005, ..., April 2005 respectively.  \n",
    "- `PAY_AMT1`, `PAY_AMT2`, ..., `PAY_AMT6`: Amount of previous payment (NT dollar) in September 2005, August 2005, ..., April 2005 respectively.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately after importing the data it was split into traning and test data. Only 75% of the data was used to train the models and the test data was only used to obtain the test performance of the model on unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we created list for numeric and categorical features, below is the summary of the traning data. It shows that that mean, standard deviation, min, max etc. The bill amount, payment amount and credit limit ranges are roughly similar which are around 800,000. It's interesting that The medians for the bill statement amounts are around 20,000, but the medians for payment amounts are 2,000. Age ranges from 21 to 75 which is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "variables": {
     "summary": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>LIMIT_BAL</th>\n      <th>AGE</th>\n      <th>BILL_AMT1</th>\n      <th>BILL_AMT2</th>\n      <th>BILL_AMT3</th>\n      <th>BILL_AMT4</th>\n      <th>BILL_AMT5</th>\n      <th>BILL_AMT6</th>\n      <th>PAY_AMT1</th>\n      <th>PAY_AMT2</th>\n      <th>PAY_AMT3</th>\n      <th>PAY_AMT4</th>\n      <th>PAY_AMT5</th>\n      <th>PAY_AMT6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>count</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>22500.00000</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>2.250000e+04</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mean</td>\n      <td>167229.763556</td>\n      <td>35.487022</td>\n      <td>50992.89800</td>\n      <td>48905.718978</td>\n      <td>46629.685644</td>\n      <td>42932.418844</td>\n      <td>39905.282444</td>\n      <td>38385.688222</td>\n      <td>5714.377733</td>\n      <td>5.848260e+03</td>\n      <td>5132.902667</td>\n      <td>4728.448311</td>\n      <td>4725.760978</td>\n      <td>5282.126533</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>std</td>\n      <td>129384.485693</td>\n      <td>9.182223</td>\n      <td>73064.68632</td>\n      <td>70748.066294</td>\n      <td>68376.985307</td>\n      <td>63802.950987</td>\n      <td>60135.853082</td>\n      <td>58733.428102</td>\n      <td>17078.235838</td>\n      <td>2.191690e+04</td>\n      <td>16892.473653</td>\n      <td>15430.720628</td>\n      <td>15138.455175</td>\n      <td>18506.384982</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>min</td>\n      <td>10000.000000</td>\n      <td>21.000000</td>\n      <td>-165580.00000</td>\n      <td>-69777.000000</td>\n      <td>-157264.000000</td>\n      <td>-170000.000000</td>\n      <td>-81334.000000</td>\n      <td>-339603.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>25%</td>\n      <td>50000.000000</td>\n      <td>28.000000</td>\n      <td>3565.75000</td>\n      <td>2928.000000</td>\n      <td>2577.000000</td>\n      <td>2313.000000</td>\n      <td>1711.750000</td>\n      <td>1190.000000</td>\n      <td>990.000000</td>\n      <td>8.000000e+02</td>\n      <td>390.000000</td>\n      <td>285.750000</td>\n      <td>238.000000</td>\n      <td>119.750000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>50%</td>\n      <td>140000.000000</td>\n      <td>34.000000</td>\n      <td>22169.00000</td>\n      <td>20859.000000</td>\n      <td>19889.000000</td>\n      <td>18855.500000</td>\n      <td>17875.000000</td>\n      <td>16715.000000</td>\n      <td>2100.000000</td>\n      <td>2.001000e+03</td>\n      <td>1800.000000</td>\n      <td>1500.000000</td>\n      <td>1500.000000</td>\n      <td>1500.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>75%</td>\n      <td>240000.000000</td>\n      <td>41.000000</td>\n      <td>66732.75000</td>\n      <td>63104.250000</td>\n      <td>59532.500000</td>\n      <td>53339.500000</td>\n      <td>49743.000000</td>\n      <td>48863.500000</td>\n      <td>5006.000000</td>\n      <td>5.000000e+03</td>\n      <td>4512.000000</td>\n      <td>4000.000000</td>\n      <td>4000.000000</td>\n      <td>4000.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>max</td>\n      <td>800000.000000</td>\n      <td>75.000000</td>\n      <td>746814.00000</td>\n      <td>743970.000000</td>\n      <td>855086.000000</td>\n      <td>616836.000000</td>\n      <td>587067.000000</td>\n      <td>568638.000000</td>\n      <td>873552.000000</td>\n      <td>1.227082e+06</td>\n      <td>889043.000000</td>\n      <td>621000.000000</td>\n      <td>426529.000000</td>\n      <td>528666.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>AGE</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22500.00</td>\n",
       "      <td>22500.00</td>\n",
       "      <td>22500.00</td>\n",
       "      <td>22500.00</td>\n",
       "      <td>22500.00</td>\n",
       "      <td>22500.00</td>\n",
       "      <td>22500.00</td>\n",
       "      <td>22500.00</td>\n",
       "      <td>22500.00</td>\n",
       "      <td>22500.00</td>\n",
       "      <td>22500.00</td>\n",
       "      <td>22500.00</td>\n",
       "      <td>22500.00</td>\n",
       "      <td>22500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>167229.76</td>\n",
       "      <td>35.49</td>\n",
       "      <td>50992.90</td>\n",
       "      <td>48905.72</td>\n",
       "      <td>46629.69</td>\n",
       "      <td>42932.42</td>\n",
       "      <td>39905.28</td>\n",
       "      <td>38385.69</td>\n",
       "      <td>5714.38</td>\n",
       "      <td>5848.26</td>\n",
       "      <td>5132.90</td>\n",
       "      <td>4728.45</td>\n",
       "      <td>4725.76</td>\n",
       "      <td>5282.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>129384.49</td>\n",
       "      <td>9.18</td>\n",
       "      <td>73064.69</td>\n",
       "      <td>70748.07</td>\n",
       "      <td>68376.99</td>\n",
       "      <td>63802.95</td>\n",
       "      <td>60135.85</td>\n",
       "      <td>58733.43</td>\n",
       "      <td>17078.24</td>\n",
       "      <td>21916.90</td>\n",
       "      <td>16892.47</td>\n",
       "      <td>15430.72</td>\n",
       "      <td>15138.46</td>\n",
       "      <td>18506.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10000.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>-165580.00</td>\n",
       "      <td>-69777.00</td>\n",
       "      <td>-157264.00</td>\n",
       "      <td>-170000.00</td>\n",
       "      <td>-81334.00</td>\n",
       "      <td>-339603.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50000.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>3565.75</td>\n",
       "      <td>2928.00</td>\n",
       "      <td>2577.00</td>\n",
       "      <td>2313.00</td>\n",
       "      <td>1711.75</td>\n",
       "      <td>1190.00</td>\n",
       "      <td>990.00</td>\n",
       "      <td>800.00</td>\n",
       "      <td>390.00</td>\n",
       "      <td>285.75</td>\n",
       "      <td>238.00</td>\n",
       "      <td>119.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>140000.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>22169.00</td>\n",
       "      <td>20859.00</td>\n",
       "      <td>19889.00</td>\n",
       "      <td>18855.50</td>\n",
       "      <td>17875.00</td>\n",
       "      <td>16715.00</td>\n",
       "      <td>2100.00</td>\n",
       "      <td>2001.00</td>\n",
       "      <td>1800.00</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>1500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>240000.00</td>\n",
       "      <td>41.00</td>\n",
       "      <td>66732.75</td>\n",
       "      <td>63104.25</td>\n",
       "      <td>59532.50</td>\n",
       "      <td>53339.50</td>\n",
       "      <td>49743.00</td>\n",
       "      <td>48863.50</td>\n",
       "      <td>5006.00</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>4512.00</td>\n",
       "      <td>4000.00</td>\n",
       "      <td>4000.00</td>\n",
       "      <td>4000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>800000.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>746814.00</td>\n",
       "      <td>743970.00</td>\n",
       "      <td>855086.00</td>\n",
       "      <td>616836.00</td>\n",
       "      <td>587067.00</td>\n",
       "      <td>568638.00</td>\n",
       "      <td>873552.00</td>\n",
       "      <td>1227082.00</td>\n",
       "      <td>889043.00</td>\n",
       "      <td>621000.00</td>\n",
       "      <td>426529.00</td>\n",
       "      <td>528666.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LIMIT_BAL       AGE   BILL_AMT1  BILL_AMT2   BILL_AMT3   BILL_AMT4  \\\n",
       "count   22500.00  22500.00    22500.00   22500.00    22500.00    22500.00   \n",
       "mean   167229.76     35.49    50992.90   48905.72    46629.69    42932.42   \n",
       "std    129384.49      9.18    73064.69   70748.07    68376.99    63802.95   \n",
       "min     10000.00     21.00  -165580.00  -69777.00  -157264.00  -170000.00   \n",
       "25%     50000.00     28.00     3565.75    2928.00     2577.00     2313.00   \n",
       "50%    140000.00     34.00    22169.00   20859.00    19889.00    18855.50   \n",
       "75%    240000.00     41.00    66732.75   63104.25    59532.50    53339.50   \n",
       "max    800000.00     75.00   746814.00  743970.00   855086.00   616836.00   \n",
       "\n",
       "       BILL_AMT5   BILL_AMT6   PAY_AMT1    PAY_AMT2   PAY_AMT3   PAY_AMT4  \\\n",
       "count   22500.00    22500.00   22500.00    22500.00   22500.00   22500.00   \n",
       "mean    39905.28    38385.69    5714.38     5848.26    5132.90    4728.45   \n",
       "std     60135.85    58733.43   17078.24    21916.90   16892.47   15430.72   \n",
       "min    -81334.00  -339603.00       0.00        0.00       0.00       0.00   \n",
       "25%      1711.75     1190.00     990.00      800.00     390.00     285.75   \n",
       "50%     17875.00    16715.00    2100.00     2001.00    1800.00    1500.00   \n",
       "75%     49743.00    48863.50    5006.00     5000.00    4512.00    4000.00   \n",
       "max    587067.00   568638.00  873552.00  1227082.00  889043.00  621000.00   \n",
       "\n",
       "        PAY_AMT5   PAY_AMT6  \n",
       "count   22500.00   22500.00  \n",
       "mean     4725.76    5282.13  \n",
       "std     15138.46   18506.38  \n",
       "min         0.00       0.00  \n",
       "25%       238.00     119.75  \n",
       "50%      1500.00    1500.00  \n",
       "75%      4000.00    4000.00  \n",
       "max    426529.00  528666.00  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1. Summary the data used in this study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the association between numeric features we explored their inter-correlations which can be seen below. \n",
    "We can observe that some features a stronger co-linearity such as BILL-AMT1,BILL-AMT2,.. to BILL-AMT6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../results/num_corr_chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1. Inter-correlation between numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also study the correlation between the features and the response varibale. We can see that some of the features have stronger correlation with the response varibale than others, for example LIMIT_BALANCE and Age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../results/num_res_chart.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[](roc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2. Correlation between numeric features and response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2 also shows that many of the features have a heavy tail distribution.  To mitigate this issue we applied [SMOTE](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html) (Synthetic Minority Oversampling Technique) on the response variable to create a balanced data set to fit the model. Furthermore, we implemented [`RobustScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) to scale predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results <a class=\"anchor\" id=\"fourth-bullet\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected logistic regression model(`LogisticRegression`) and [`RFE`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE)(recursive feature elimination) as our model since it is more robust given that the dataset has many of the features are not normally distributed. One additional advantage of (`LogisticRegression`) that is much interpretable than more complex models\n",
    "\n",
    "We started the analysis by applying a robust scalar on the training data-set. Following that we build a model with the full set of features as our base-case model. The confusion matrix, evaluation matrix and ROC results were obtained to set the a bench-mark for comparison purposes. `RFE` was then used to identify the most useful predictors and consequently we dropped those columns that are deemed as less useful. Eventually 7 features were used to train the model.\n",
    "\n",
    "The hyperparameters `C` was tunned in the range from -4 to 20 using 5-fold cross-validation and the model was then fitted with the best hyperparameter. Let us now look at the result by glancing into the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best-case Model            |  Base-case Model\n",
    ":-------------------------:|:-------------------------:\n",
    "![](../results/confusion_matrix.png)  |  ![](../results_baseline/confusion_matrix.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3. Confusion matrix of the fitted model with 7 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best-case model which uses 7 features out-performs the base-case model in many aspects. First, it has obtained more true negatives on the test data than the base-case model. Furthermore, the best-case model has less false positives than the base-case model making it more precise as depicted in the evaluation matrix below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measurement</th>\n",
       "      <th>baseline</th>\n",
       "      <th>alternate model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test accuracy</td>\n",
       "      <td>0.670267</td>\n",
       "      <td>0.700400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train accuracy</td>\n",
       "      <td>0.670933</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test recall</td>\n",
       "      <td>0.664653</td>\n",
       "      <td>0.625378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test precision</td>\n",
       "      <td>0.364480</td>\n",
       "      <td>0.388805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>auc score</td>\n",
       "      <td>0.720814</td>\n",
       "      <td>0.715227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      measurement  baseline  alternate model\n",
       "0   test accuracy  0.670267         0.700400\n",
       "1  train accuracy  0.670933         0.695200\n",
       "2     test recall  0.664653         0.625378\n",
       "3  test precision  0.364480         0.388805\n",
       "4       auc score  0.720814         0.715227"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 2. Comparison of the evaluation matrix between models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "auc": "0.72",
     "precision": "0.39",
     "precision_base": "0.36",
     "recall": "0.63",
     "recall_base": "0.66",
     "test_accuracy": "0.7",
     "test_accuracy_base": "0.67"
    }
   },
   "source": [
    "Evaluation matrix in Table 2 shows the accuracy of the alternate model on test data was about {{test_accuracy}} compared with only {{test_accuracy_base}} for the baseline model. The recall on test data dropped slightly to {{recall}} compared with recall of the baseline which is {{recall_base}}. The precision for the model on the test data has improved {{precision}} compared with only {{precision_base}} for the baseline model. The area under the ROC Curve for the final model is {{auc}} which is comparable to the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC was plotted to measure the model's discriminative ability. The dashed diagonal line represents a model that labelling observations randomly. The further away from the diagonal line towards top left corner, the better the model can distinguish two classes - default, non-default customers correctly.  The blue line is our best model with 7 features, we can see that the model performs fairly good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../results/roc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4. ROC curve for the fitted model with 7 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "precision": "0.39",
     "precision_base": "0.36",
     "recall": "0.63",
     "recall_base": "0.66"
    }
   },
   "source": [
    "# 5. Conclusions <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "\n",
    "We were able to successfully use `LogisticRegression` model to find the most important features that predict customer default. The model acheives an acceptable level of accuracy on the testing data, better tunning of hyper paramters may result a higher accuracy. Overall, we selected the best-case model to extract the most important features as it is more accurate. The precision of the best-case model is  {{precision}}. In comparison, the base-case model only scores  {{precision_base}}. While the recall of best-case model decreased from {{recall_base}} to {{recall}}, AUC score only slightly dropped.  Since the best-case model is more accurate, we expect the following 7 features to have the highest predictive power among all the features\n",
    "\n",
    "1. Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
    "2. EDUCATION\n",
    "3. MARRIAGE\n",
    "4. AGE\n",
    "5. Past monthly repayment status in September 2005 (which is the most recent month before the prediction month)\n",
    "6. Past monthly repayment status in August 2005 (which is the second from the most recent month before the predition month)\n",
    "7. Amount of previous payment (NT dollar) in September 2005 (which is the most recent month before the prediction month)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "test_accuracy": "0.7"
    }
   },
   "source": [
    "Although the best model are doing better than the baseline model overall, the result is still not very satisfactory with {{test_accuracy}} as our highest accuracy from test accuracy. To improve accuracy in the future, we have some suggentions that are not yet implemented due to time limitation. \n",
    "\n",
    "- Use some other feature scaling techniques: one-hot encoding the categorical features.\n",
    "- Use pipeline to be able to grid search the best combinations of values of LogisticRegression model parameters and number of features to select.\n",
    "- Use L1 regularization to eliminate features.  \n",
    "\n",
    "Another limitation is how to generalize the characteristics of feature 5,6,7. It makes sense the most months' repayment status and amount would be a good indicator of whether the customer will default or not in the next month. However, how well can it predict if the customer will default or not in half a year or longer is explorable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. [UCI Machine Learning Repository Irvine, CA: University of California, School of Information and Computer Science](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)\n",
    "\n",
    "[2] [Guido Van Rossum and Fred L. Drake. Python 3 Reference Manual. CreateSpace, Scotts Valley, CA, 2009](https://dl.acm.org/doi/book/10.5555/1593511)\n",
    "\n",
    "[3] Wickham, H. 2017. tidyverse: Easily Install and Load the ‘Tidyverse’. R package version 1.2.1. https://CRAN.R-project.org/package=tidyverse\n",
    "\n",
    "[4] Wickham H (2011). “testthat: Get Started with Testing.” The R Journal, 3, 5–10. https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.\n",
    "\n",
    "[5] McKinney, W. (2012). Python for data analysis: Data wrangling with Pandas, NumPy, and IPython. \" O'Reilly Media, Inc.\".\n",
    "\n",
    "[6] Nielsen, F. Å. (2014). Python programming—Scripting.\n",
    "\n",
    "[7] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & Vanderplas, J. (2011). Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct), 2825-2830.\n",
    "\n",
    "[8] VanderPlas, J., Granger, B., Heer, J., Moritz, D., Wongsuphasawat, K., Satyanarayan, A., ... & Sievert, S. (2018). Altair: Interactive statistical visualizations for python. Journal of open source software, 3(32), 1057.\n",
    "\n",
    "[9] Percival, H. (2014). Test-driven development with Python: obey the testing goat: using Django, Selenium, and JavaScript. \" O'Reilly Media, Inc.\".\n",
    "\n",
    "[10] Lemaître, G., Nogueira, F., & Aridas, C. K. (2017). Imbalanced-learn: A python toolbox to tackle the curse of imbalanced datasets in machine learning. The Journal of Machine Learning Research, 18(1), 559-563.\n",
    "\n",
    "[11] Li, Susan. \"Building A Logistic Regression in Python, Step by Step.\" Towards Data Science (2017). https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cite data-cite=\"Python\"></cite>\n",
    "<cite data-cite=\"Dua:2019\"></cite>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
